---
---

@string{aps = {American Physical Society,}}

@article{momentum,
  abbr={NeurIPS},
  title={MomentumSMoE: Integrating Momentum into Sparse Mixture of Experts},
  author={Rachel S.Y. Teo and Tan M. Nguyen},
  abstract={Sparse Mixture of Experts (SMoE) has become the key to unlocking unparalleled scalability in deep learning. SMoE has the potential to exponentially increase parameter count while maintaining the efficiency of the model by only activating a small subset of these parameters for a given sample. However, it has been observed that SMoE suffers from unstable training and has difficulty adapting to new distributions, leading to the model's lack of robustness to data contamination. To overcome these limitations, we first establish a connection between the dynamics of the expert representations in SMoEs and gradient descent on a multi-objective optimization problem. Leveraging our framework, we then integrate momentum into SMoE and propose a new family of SMoEs named MomentumSMoE. We theoretically prove and numerically demonstrate that MomentumSMoE is more stable and robust than SMoE. In particular, we verify the advantages of MomentumSMoE over SMoE on a variety of practical tasks including ImageNet-1K object recognition and WikiText-103 language modeling. We demonstrate the applicability of MomentumSMoE to many types of SMoE models, including those in the Sparse MoE model for vision (V-MoE) and the Generalist Language Model (GLaM). We also show that other advanced momentum-based optimization methods, such as Adam, can be easily incorporated into the MomentumSMoE framework for designing new SMoE models with even better performance, almost negligible additional computation cost, and simple implementations.},
  journal={Conference on Neural Information Processing Systems (NeurIPS), 2024},
  year={2024},
  month={Oct},
  url={https://arxiv.org/abs/2410.14574},
  pdf={2410.14574v1.pdf},
  dimensions={true},
}

@article{kpca,
  abbr={NeurIPS},
  title={Unveiling the Hidden Structure of Self-Attention via Kernel Principal Component Analysis},
  author={Rachel S.Y. Teo and Tan M. Nguyen},
  abstract={The remarkable success of transformers in sequence modeling tasks, spanning various applications in natural language processing and computer vision, is attributed to the critical role of self-attention. Similar to the development of most deep learning models, the construction of these attention mechanisms rely on heuristics and experience. In our work, we derive self-attention from kernel principal component analysis (kernel PCA) and show that self-attention projects its query vectors onto the principal component axes of its key matrix in a feature space. We then formulate the exact formula for the value matrix in self-attention, theoretically and empirically demonstrating that this value matrix captures the eigenvectors of the Gram matrix of the key vectors in self-attention. Leveraging our kernel PCA framework, we propose Attention with Robust Principal Components (RPC-Attention), a novel class of robust attention that is resilient to data contamination. We empirically demonstrate the advantages of RPC-Attention over softmax attention on the ImageNet-1K object classification, WikiText-103 language modeling, and ADE20K image segmentation task.},
  journal={Conference on Neural Information Processing Systems (NeurIPS), 2024},
  year={2024},
  month={Oct},
  url={https://arxiv.org/abs/2406.13762},
  pdf={2406.13762v1.pdf},
  dimensions={true},
}

@article{ellip,
  abbr={NeurIPS},
  title={Elliptical Attention},
  author={Stefan Nielsen* and Laziz Abdullaev* and Rachel S.Y. Teo and Tan M. Nguyen},
  abstract={Pairwise dot-product self-attention is key to the success of transformers that achieve state-of-the-art performance across a variety of applications in language and vision. This dot-product self-attention computes attention weights among the input tokens using Euclidean distance, which makes the model prone to representation collapse and vulnerable to contaminated samples. In this paper, we propose using a Mahalanobis distance metric for computing the attention weights to stretch the underlying feature space in directions of high contextual relevance. In particular, we define a hyper-ellipsoidal neighborhood around each query to increase the attention weights of the tokens lying in the contextually important directions. We term this novel class of attention Elliptical Attention. Our Elliptical Attention provides two benefits: 1) reducing representation collapse, and 2) enhancing the modelâ€™s robustness as the Elliptical Attention pays more attention to contextually relevant information, rather than focusing on some small subset of informative features. We empirically demonstrate the advantages of Elliptical Attention over the baseline dot-product attention and state-of-the-art attention methods on various practical tasks, including object classification, image segmentation, and language modeling across different data modalities.},
  journal={Conference on Neural Information Processing Systems (NeurIPS), 2024},
  year={2024},
  month={Oct},
  url={https://arxiv.org/abs/2406.13770},
  pdf={2406.13770v1.pdf},
  dimensions={true},
}
